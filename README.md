# Projeto Engenharia de Dados
## ğŸš€ Data Pipeline Project

Este repositÃ³rio contÃ©m a estrutura de um projeto completo de pipeline de dados, integrando o Apache Airflow e dbt (Data Build Tool), bem como a utilizaÃ§Ã£o de spark na extraÃ§Ã£o dos dados, criaÃ§Ã£o de um storage utilizando o Minio e a utilizaÃ§Ã£o dos bancos de dados Postgres e MariaDb. O objetivo deste projeto Ã© fornecer uma base introdutÃ³ria, porÃ©m sÃ³lida para a construÃ§Ã£o, orquestraÃ§Ã£o e transformaÃ§Ã£o de dados, utilizando as melhores prÃ¡ticas do setor, bem como a entrega prÃ¡tica dos meus estudos sobre a Ã¡rea de engenharia de dados. Tive como base de inspiraÃ§Ã£o algumas publicaÃ§Ãµes nessa Ã¡rea, os cursos(links no final) e experiÃªncias de trabalho que obtive ao longo dos Ãºltimos anos. 
AlÃ©m dessa parte mais tecnica, deixo aqui uma publicaÃ§Ã£o que fiz, tendo como tÃ­tulo "Aspectos essenciais e PrÃ¡ticos sobre Engenharia de Dados"(Link)

Espero que todo esse material possa servir de base para todos aqueles que queiram ingressar ou se aprofundar mais nessa area. 
Bons estudos e bebam Ã¡guağŸ’¦!

## ğŸ“Š Arquitetura do Pipeline
Abaixo estÃ¡ a representaÃ§Ã£o grÃ¡fica da arquitetura deste pipeline de dados:

![Arquitetura do Pipeline](https://github.com/user-attachments/assets/4547f48f-18d5-41e4-8938-c624b6e43a56)

Nesta arquitetura, os dados sÃ£o extraÃ­dos de vÃ¡rias fontes, transformados e carregados em um Data Warehouse utilizando Apache Airflow e dbt, e finalmente consumidos por ferramentas de visualizaÃ§Ã£o como o Metabase.

## ğŸ“‚ Estrutura do Projeto
A estrutura do projeto estÃ¡ organizada da seguinte maneira:

```
/ENG_DADOS_PROJETO1
â”‚
â”œâ”€â”€ .temp/                             # Nessa pasta existe os nossos arquivos iniciais do estudo de extraÃ§Ã£o de dados(Videos no youtube)
â”‚   â”œâ”€â”€ dag_csv.py
â”‚   â””â”€â”€ task_csv.py
â”‚   â””â”€â”€ task_parquet_full.py
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ config_airflow/
â”‚   â”‚   â””â”€â”€ airflow.Dockerfile         # Dockerfile customizado para o Airflow
â”‚   â”‚   â””â”€â”€ credentials.json           # Credenciais que utilizamos na API do google sheets(por questÃµes de seguranÃ§a o #github nÃ£o permite a submissÃ£o desse tipo de arquivo)
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ dag_main.py                # Arquivo principal da DAG contendo as extraÃ§Ãµes e as transformaÃ§Ãµes em dbt.
â”‚   â”œâ”€â”€ tasks/
â”‚   â”‚   â”œâ”€â”€ task_sheets.py             # Arquivo de task contendo a extraÃ§Ã£o dos dados vindos do google sheets
â”‚   â”‚   â””â”€â”€ task_parquet.py            # Arquivo de task contendo a extraÃ§Ã£o dos dados vindos do postgres(Projeto feito atraves do curso do Prof. Fernando Amaral)
â”œâ”€â”€ dbt_bike/                          # Nessa pasta existe as transformaÃ§Ãµes de dados vindos do google sheets, aqui utilizamos o BD da empresa "Bikes_Ronaldinho"(nome criado carinhosamente pela minha filha)
â”‚   â”œâ”€â”€ dbt_project.yml                # Arquivo de configuraÃ§Ã£o principal do dbt
â”‚   â”œâ”€â”€ profiles.yml                   # ConfiguraÃ§Ã£o de perfil para conectar ao banco de dados mariadb
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ dw_dim/
â”‚   â”‚   â”‚   â””â”€â”€ dim_cliente_bike.sql   # Modelos de tabelas de dimessÃ£o
â”‚   â”‚   â”‚   â””â”€â”€ schemas.sql            # CriaÃ§Ã£o do schema de mapeamento das tabelas e colunas que coletamos da nossa camada landing 
â”‚   â”‚   â””â”€â”€ dw_fatos/
â”‚   â”‚   â”‚   â””â”€â”€ fatos_vendas_bike.sql  # Modelos de tabelas de fatos
â”‚   â”‚   â”‚   â””â”€â”€ schemas.sql            # CriaÃ§Ã£o do schema de mapeamento das tabelas e colunas que coletamos da nossa camada landing 
â”‚   â”œâ”€â”€ dbt_packages/                  # Arquivos de pacotes a serem utilizados pelo proprio dbt         
â”‚   â””â”€â”€ seeds/
â”‚       â””â”€â”€ seed_data.csv              # Dados de seed para prÃ©-carregar no dbt, utilizados caso fossemos incluir dados vindos em csv(nÃ£o fizemos isso!)
â”œâ”€â”€ dbt_novadrive_dw/                  # Nessa pasta existe as transformaÃ§Ãµes de dados vindos do postgres, aqui utilizamos o BD da empresa NovaDrive(projeto usado no curso)
â”‚   â”œâ”€â”€ dbt_project.yml                # Arquivo de configuraÃ§Ã£o principal do dbt
â”‚   â”œâ”€â”€ profiles.yml                   # ConfiguraÃ§Ã£o de perfil para conectar ao banco de dados mariadb
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ dw_dim/
â”‚   â”‚   â”‚   â””â”€â”€ dim_clientes.sql       # Modelos de tabelas de dimessÃ£o
â”‚   â”‚   â”‚   â””â”€â”€ schemas.sql            # CriaÃ§Ã£o do schema de mapeamento das tabelas e colunas que coletamos da nossa camada landing 
â”‚   â”‚   â””â”€â”€ dw_fatos/
â”‚   â”‚   â”‚   â””â”€â”€ fatos_vendas.sql       # Modelos de tabelas de fatos
â”‚   â”‚   â”‚   â””â”€â”€ schemas.sql            # CriaÃ§Ã£o do schema de mapeamento das tabelas e colunas que coletamos da nossa camada landing 
â”‚   â”œâ”€â”€ dbt_packages/                  # Arquivos de pacotes a serem utilizados pelo proprio dbt         
â”‚   â””â”€â”€ seeds/
â”‚       â””â”€â”€ seed_data.csv              # Dados de seed para prÃ©-carregar no dbt, utilizados caso fossemos incluir dados vindos em csv(nÃ£o fizemos isso!)    
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ .gitgnore
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
```

## ğŸ› ï¸ Tecnologias Utilizadas 
- **Apache Airflow**: Para orquestraÃ§Ã£o de workflows e automaÃ§Ã£o de tarefas. 
- **dbt (Data Build Tool)**: Para transformaÃ§Ã£o de dados, modelagem e criaÃ§Ã£o de tabelas no data warehouse. 
- **Docker**: Para containerizaÃ§Ã£o de serviÃ§os, garantindo um ambiente isolado e reprodutÃ­vel.
- **Spark**: Utilizado para processamento de grandes volumes de dados de maneira distribuÃ­da(aqui utilizamos para realizar a extraÃ§Ã£o dos dados). 
- **MariaDB**: Banco de dados utilizado como Data Warehouse para armazenar as tabelas de dimensÃµes e fatos. 
- **Metabase**: Ferramenta de BI para visualizaÃ§Ã£o e anÃ¡lise dos dados armazenados no Data Warehouse. 


## ğŸ³ Docker
O projeto estÃ¡ configurado para rodar em um ambiente Docker. O `docker-compose.yaml` e o `Dockerfile` na raiz do projeto sÃ£o usados para configurar o ambiente de desenvolvimento e execuÃ§Ã£o dos serviÃ§os. AlÃ©m disso, o Airflow possui um `Dockerfile` customizado para garantir que todas as dependÃªncias especÃ­ficas sejam atendidas.

## ğŸ“„ Airflow
- **DAGs**: As DAGs (Directed Acyclic Graphs) sÃ£o definidas dentro da pasta `airflow/dags/`. O arquivo principal Ã© o `dag_main.py`, que orquestra as diferentes tarefas.
- **Tasks**: As tarefas sÃ£o modularizadas dentro da pasta `airflow/tasks/`. Um exemplo Ã© o `task_parquet.py`, que pode conter lÃ³gica para processar arquivos parquet.
- **ConfiguraÃ§Ãµes**: Todas as configuraÃ§Ãµes e customizaÃ§Ãµes especÃ­ficas do Airflow estÃ£o na pasta `airflow/config_airflow/`.

## ğŸ—ï¸ dbt
- **ConfiguraÃ§Ã£o**: O arquivo `dbt_project.yml` contÃ©m as configuraÃ§Ãµes principais do dbt, enquanto o `profiles.yml` Ã© usado para definir os perfis de conexÃ£o com os bancos de dados.
- **Modelos**: Os modelos de dados sÃ£o organizados em pastas como `dim_tables`, e `fatos_tables`. Esses arquivos SQL definem as tabelas e vistas no data warehouse.
- **Macros**: FunÃ§Ãµes personalizadas que podem ser usadas nos modelos dbt estÃ£o em `macros/custom_macros.sql`.

## ğŸš€ Como ComeÃ§ar

1. Clone o repositÃ³rio:
   ```bash
   git clone https://github.com/seu-usuario/eng_dados_projeto1.git
   ```
2. Navegue atÃ© o diretÃ³rio do projeto:
   ```bash
   cd eng_dados_projeto1
   ```
3. Suba os containers com Docker:
   ```bash
   docker-compose up -d
   ```
4. Acesse o Airflow na URL `http://localhost:8080` e inicie as DAGs conforme necessÃ¡rio.

## ğŸ“š DocumentaÃ§Ã£o

- [DocumentaÃ§Ã£o Oficial do Airflow](https://airflow.apache.org/docs/)
- [DocumentaÃ§Ã£o Oficial do dbt](https://docs.getdbt.com/)
- [DocumentaÃ§Ã£o Oficial do Docker](https://docs.docker.com)
- [DocumentaÃ§Ã£o Oficial do Metabase](https://www.metabase.com/docs/latest/)

## ğŸ“‹ ContribuiÃ§Ãµes e Duvidas

ContribuiÃ§Ãµes e duvidas sÃ£o bem-vindas, qualquer coisa manda msg!

## ğŸ“ LicenÃ§a

Este projeto estÃ¡ licenciado sob a [MIT License](LICENSE).
